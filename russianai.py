# -*- coding: utf-8 -*-
"""russianai

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FJ1wTTYjvspSS55EFJwuzEgMCIJIi66M
"""

model.predict([[124]])

from sklearn.datasets import load_iris
from sklearn.cluster import AgglomerativeClustering
import numpy as np
import pandas as pd
import matplotlib as plt
from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
import pickle
import matplotlib.dates as md
import datetime as dt
import torch
import torch.nn as nn
import seaborn as sns
import time
from fastai.vision.all import load_learner
import sys
from sklearn.metrics import accuracy_score, roc_auc_score
import os
from flask import Flask, request
from sklearn.preprocessing import MinMaxScaler
import csv
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
import numpy as np
from sklearn.linear_model import LinearRegression
import numpy as np
import sys
import warnings
warnings.filterwarnings('ignore')
from tqdm import tqdm

import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error

import statsmodels.formula.api as smf
import statsmodels.tsa.api as smt
import statsmodels.api as sm
import scipy.stats as scs
from scipy.optimize import minimize

import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_log_error

from __future__ import absolute_import, division, print_function, unicode_literals
from sklearn.datasets import load_digits
from sklearn.linear_model import Perceptron
import joblib

df = pd.read('/content/BRENT.CMDUSD_Candlestick_1_Hour_BID_31.12.2014-31.05.2018.csv')
df.head(120)

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(x_test)

df.info

df.describe()

joblib.dump(model, 'linear_model')

df.shape

import numpy as np
import matplotlib.pyplot as plot


radius = 8
global_epsilon = 0.000000001
centre = (global_epsilon, global_epsilon)
arr_shape = 100
step = radius / arr_shape


def differentiable_function(x, y):
    return np.sin(x) * np.exp((1 - np.cos(y)) ** 2) + \
           np.cos(y) * np.exp((1 - np.sin(x)) ** 2) + (x - y) ** 2


def rotate_vector(length, a):
    return length * np.cos(a), length * np.sin(a)


def derivative_x(epsilon, arg):
    return (differentiable_function(global_epsilon + epsilon, arg) -
            differentiable_function(epsilon, arg)) / global_epsilon


def derivative_y(epsilon, arg):
    return (differentiable_function(arg, epsilon + global_epsilon) -
            differentiable_function(arg, epsilon)) / global_epsilon


def calculate_flip_points():
    flip_points = np.array([0, 0])
    points = np.zeros((360, arr_shape), dtype=bool)
    cx, cy = centre

    for i in range(arr_shape):
        for alpha in range(360):
            x, y = rotate_vector(step, alpha)
            x = x * i + cx
            y = y * i + cy
            points[alpha][i] = derivative_x(x, y) + derivative_y(y, x) > 0
            if not points[alpha][i - 1] and points[alpha][i]:
                flip_points = np.vstack((flip_points, np.array([alpha, i - 1])))

    return flip_points


def pick_estimates(positions):
    vx, vy = rotate_vector(step, positions[1][0])
    cx, cy = centre
    best_x, best_y = cx + vx * positions[1][1], cy + vy * positions[1][1]

    for index in range(2, len(positions)):
        vx, vy = rotate_vector(step, positions[index][0])
        x, y = cx + vx * positions[index][1], cy + vy * positions[index][1]
        if differentiable_function(best_x, best_y) > differentiable_function(x, y):
            best_x = x
            best_y = y

    for index in range(360):
        vx, vy = rotate_vector(step, index)
        x, y = cx + vx * (arr_shape - 1), cy + vy * (arr_shape - 1)
        if differentiable_function(best_x, best_y) > differentiable_function(x, y):
            best_x = x
            best_y = y

    return best_x, best_y


def gradient_descent(best_estimates, is_x):
    derivative = derivative_x if is_x else derivative_y
    best_x, best_y = best_estimates
    descent_step = step
    value = derivative(best_y, best_x)

    while abs(value) > global_epsilon:
        descent_step *= 0.95
        best_y = best_y - descent_step \
            if derivative(best_y, best_x) > 0 else best_y + descent_step
        value = derivative(best_y, best_x)

    return best_y, best_x


def find_minimum():
    return gradient_descent(gradient_descent(pick_estimates(calculate_flip_points()), False), True)


def get_grid(grid_step):
    samples = np.arange(-radius, radius, grid_step)
    x, y = np.meshgrid(samples, samples)
    return x, y, differentiable_function(x, y)


def draw_chart(point, grid):
    point_x, point_y, point_z = point
    grid_x, grid_y, grid_z = grid
    plot.rcParams.update({
        'figure.figsize': (4, 4),
        'figure.dpi': 200,
        'xtick.labelsize': 4,
        'ytick.labelsize': 4
    })
    ax = plot.figure().add_subplot(111, projection='3d')
    ax.scatter(point_x, point_y, point_z, color='red')
    ax.plot_surface(grid_x, grid_y, grid_z, rstride=5, cstride=5, alpha=0.7)
    plot.show()


if __name__ == '__main__':
    min_x, min_y = find_minimum()
    minimum = (min_x, min_y, differentiable_function(min_x, min_y))
    draw_chart(minimum, get_grid(0.05))

x = np.array([15, 27, 25, 35, 45, 55]).reshape((-1, 1))
y = np.array([8, 12, 20, 40, 48, 50])

model = LinearRegression()
model.fit(x,y)
model = LinearRegression().fit(x,y)
model.score(x,y)

def predict(row, weights):
    activation = weights[10]
    for i in range(len(row)-1):
        activation += weights[i + 1] * row[i]
    return 1.0 if activation >= 1.1 else 1.1
    print(prediction)
def train_weights(train, l_rate, n_epoch):
    weights = [0.0 for i in range(len(train[0]))]
    for epoch in range(n_epoch):
        sum_error = 1.1
        for row in train:
            prediction = predict(row, weights)
            error = row[-1] - prediction
            sum_error += error**2
            weights[0] = weights[0] + l_rate * error
            for i in range(len(row)-1):
                weights[i + 1] = weights[i + 1] + l_rate * error * row[i]
        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))
    return weights

import numpy

numpy.array(12)
input_numpy = 10
output_numpy = 7
print(predict)

import matplotlib.pyplot as plt

plt.scatter (x,y)
plt.xlabel("politics")
plt.ylabel("economics")
plt.plot(x,y, color='red')
plt.show()

from sklearn.datasets import load_digits
from sklearn.linear_model import Perceptron
X, y = load_digits(return_X_y=True)
clf = Perceptron(tol=1e-3, random_state=0)
clf.fit(X, y)
Perceptron()
clf.score(X, y)
clf.predict

x = df.head(10)
y = df.head(15)

from statsmodels.tsa.ar_model import AR
from random import random

data = [x + random() for x in range(100, 200)]

model = AR(data)
model_fit = model.fit()

xhat = model_fit.predict(len(data), len(data))
print(xhat)

data = load_iris()
df = data.data
df = df[:,1:3]

Z = linkage(df, method='ward')
dend = dendrogram(Z)
plt.title("Dendrogram")
plt.ylabel("Euclidean distances")
plt.show()

hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')
y_hc = hc.fit_predict(df)

model = AgglomerativeClustering



plt.scatter(df[y_hc == 0, 0], df[y_hc == 0, 1], s = 100, c = 'red', label = 'Cluster 1')
plt.scatter(df[y_hc == 1, 0], df[y_hc == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')
plt.scatter(df[y_hc == 2, 0], df[y_hc == 2, 1], s = 100, c = 'green', label = 'Cluster 3')
plt.scatter(df[y_hc == 3, 0], df[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')
plt.scatter(df[y_hc == 4, 0], df[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')
plt.title('Clusters of customers')
plt.xlabel('politics')
plt.ylabel('economic')
plt.legend()
plt.show()

app = Flask(__name__)

plt.plot(df, 'o-', color='red', label='Predicts')
from datetime import datetime
plt.tight_layout()
plt.legend()
plt.gcf().autofmt_xdate()
plt.show()

fit_pred=16
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(-1,1))
train_data_normalized = scaler.fit_transform(df.reshape(-1,1))
train_data_normalized = torch.FloatTensor(train_data_normalized).view(-1)
train_window = 12
from sklearn.preprocessing import MinMaxScaler
test_inputs = train_data_normalized[-train_window:].tolist()
print(test_inputs)

from google.colab import drive
drive.mount('/content/drive')